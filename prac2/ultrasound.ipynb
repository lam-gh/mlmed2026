{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39804fd6-5f1c-473a-aa94-844ce766856f",
   "metadata": {},
   "source": [
    "# Overview (Schizo Edition)\n",
    "\n",
    "Find the head circumference given ultrasound image.\n",
    "\n",
    "> How do we do this? \n",
    "\n",
    "In the `data/hc18` directory, we have the dataset with images, annotated images, pixel size (to show how many mm each pixel occupies) and the actual head circumference. \n",
    "\n",
    "> How do we find the head circumference?\n",
    "\n",
    "We draw the segment mask on the ultrasound image using some deep learning model, then we estimate an ellipse on the mask using opencv, and finally we calculate the head circumference by finding the circumference of the estimated ellipse and the given pixel size. \n",
    "\n",
    "> How do we draw a segment mask on the ultrasound image?\n",
    "\n",
    "We will use PyTorch, use the data in the `training_set` as the input, where the original ultrasound image is the input, and the annotated binary mask is the output of the model. \n",
    "\n",
    "> Cool, then how do we train the model in question using PyTorch?  \n",
    "\n",
    "reference: https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/\n",
    "\n",
    "We will use U-Net architecture.\n",
    "\n",
    "1. Prepare the data!\n",
    "   * We don't need to do much, since the images here are already single channel (gray-scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df3021f-5f5e-4274-8837-c132c58bd6ab",
   "metadata": {},
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72d2befd-02c9-4fb4-8cc6-72c35f18584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_class import SegmentationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d28e211-2092-4684-b889-b045e53cf534",
   "metadata": {},
   "source": [
    "# U-Net model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8752e-8c89-476f-a301-49f6e99bcba9",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b548aa42-208a-48b8-b4e1-8846d34ba79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "\n",
    "DATASET_PATH = os.path.join(\"../data\", \"hc18\") \n",
    "\n",
    "IMAGE_DATASET_PATH = os.path.join(DATASET_PATH, \"training_set\")\n",
    "# No need for mask path, we will filter from the same dir \n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "DEVICE = \"mps\" if torch.mps.is_available() else \"cpu\" # because I'm on Mac or shitty laptop\n",
    "\n",
    "# No pin memory. Didn't work. \n",
    "# ref: https://github.com/pytorch/pytorch/issues/86060\n",
    "\n",
    "NUM_CHANNELS = 1 \n",
    "NUM_CLASSES = 1 \n",
    "NUM_LEVELS = 3 \n",
    "\n",
    "INIT_LR = 0.001 \n",
    "NUM_EPOCHS = 40 \n",
    "BATCH_SIZE = 8 \n",
    "\n",
    "INPUT_IMAGE_WIDTH = 800 \n",
    "INPUT_IMAGE_HEIGHT = 540 \n",
    "\n",
    "THRESHOLD = 0.5 \n",
    "\n",
    "BASE_OUTPUT = \"output\" \n",
    "\n",
    "MODEL_PATH = os.path.join(BASE_OUTPUT, \"unet_hc.pth\") \n",
    "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n",
    "TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f489550-d5cc-4744-8412-50b4b2656382",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2725e87-285f-4c68-b147-ff85ff4b7db0",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbcb0319-859f-494c-a14c-d151576f56a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import ConvTranspose2d \n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn import ReLU\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torch.nn import functional as F "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd992d3-c4cf-4ada-8f83-821883a60812",
   "metadata": {},
   "source": [
    "### Block module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95680e4f-3f0e-4a19-97de-8c027f0387ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(Module): \n",
    "    def __init__(self, inChannels, outChannels): \n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2d(inChannels, outChannels, 3)\n",
    "        self.relu = ReLU() \n",
    "        self.conv2 = Conv2d(outChannels, outChannels, 3)\n",
    "\n",
    "    def forward(self, x): \n",
    "        # Ugliest syntax award. \n",
    "        # Would look better with piping (functional)\n",
    "        return self.conv2(self.relu(self.conv1(x))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024c0092-69de-4fbc-bf49-4c925e9bcc45",
   "metadata": {},
   "source": [
    "### Encoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fccff9dc-fad8-4916-9fca-00ce65857dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "    def __init__(self, channels=(1, 16, 32, 64)):\n",
    "        super().__init__()\n",
    "        self.encBlocks = ModuleList(\n",
    "            [Block(channels[i], channels[i + 1]) for i in range(len(channels) - 1)]\n",
    "        )\n",
    "        self.pool = MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x): \n",
    "        blockOutputs = []\n",
    "\n",
    "        for block in self.encBlocks:\n",
    "            x = block(x)\n",
    "            blockOutputs.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        return blockOutputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02b07a-2027-44d5-929e-f3e6fccc693e",
   "metadata": {},
   "source": [
    "### Decoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97d78308-1905-432f-93ca-f3135d5bde85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Module):\n",
    "    def __init__(self, channels=(64, 32, 16)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.upconvs = ModuleList(\n",
    "            [ConvTranspose2d(channels[i], channels[i+1], 2, 2) for i in range(len(channels) - 1)]\n",
    "        )\n",
    "        self.dec_blocks = ModuleList(\n",
    "            [Block(channels[i], channels[i + 1]) for i in range(len(channels) - 1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, encFeatures):\n",
    "        for i in range(len(self.channels) - 1):\n",
    "            x = self.upconvs[i](x) \n",
    "\n",
    "            encFeat = self.crop(encFeatures[i], x)\n",
    "            x = torch.cat([x, encFeat], dim=1)\n",
    "            x = self.dec_blocks[i](x) \n",
    "\n",
    "        return x \n",
    "\n",
    "    def crop(self, encFeatures, x):\n",
    "        (_, _, H, W) = x.shape\n",
    "        encFeatures = CenterCrop([H, W])(encFeatures)\n",
    "\n",
    "        return encFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59c173-1a83-458d-8e8b-cc1d7406b1c2",
   "metadata": {},
   "source": [
    "### UNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bfff8e8-d35e-42a2-8af0-17b1ac847468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(Module): \n",
    "    def __init__(\n",
    "        self, \n",
    "        encChannels=(1, 16, 32, 64), \n",
    "        decChannels=(64, 32, 16), \n",
    "        nbClasses=1, \n",
    "        retainDim=True,\n",
    "        outSize=(INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(encChannels)\n",
    "        self.decoder = Decoder(decChannels) \n",
    "\n",
    "        self.head = Conv2d(decChannels[-1], nbClasses, 1)\n",
    "        self.retainDim = retainDim\n",
    "        self.outSize = outSize\n",
    "\n",
    "    def forward(self, x):\n",
    "        encFeatures = self.encoder(x) \n",
    "        decFeatures = self.decoder(encFeatures[::-1][0], encFeatures[::-1][1:])\n",
    "        mapper = self.head(decFeatures)\n",
    "\n",
    "        if self.retainDim: \n",
    "            mapper = F.interpolate(mapper, self.outSize)\n",
    "\n",
    "        return mapper "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f79d19d-8602-4dcb-86ec-b71bf19b6307",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6543faf4-046d-4b21-a3dc-561ae72bff29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam \n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms \n",
    "from imutils import paths \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import time \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "allPaths = sorted(list(paths.list_images(IMAGE_DATASET_PATH)))\n",
    "maskPaths = [file for file in allPaths if \"Annotation\" in file]\n",
    "imagePaths = list(set(allPaths) - set(maskPaths))\n",
    "\n",
    "# print(imagePaths)\n",
    "# print(maskPaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79a5a9c5-4853-4449-a4ef-64047875e24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving testing image paths...\n"
     ]
    }
   ],
   "source": [
    "(trainImages, testImages, trainMasks, testMasks) = train_test_split(imagePaths, maskPaths, test_size=TEST_SPLIT, random_state=67) # Six seveeeeeeeeeeen\n",
    "\n",
    "print(\"[INFO] saving testing image paths...\")\n",
    "f = open(TEST_PATHS, \"w\")\n",
    "f.write(\"\\n\".join(testImages))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9d938ba-d76f-447f-af8b-b3c0189d282a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 799 examples in the training set...\n",
      "[INFO] found 200 examples in the test set...\n"
     ]
    }
   ],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(), \n",
    "    transforms.Resize((INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH)), \n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "trainDataset = SegmentationDataset(\n",
    "    imagePaths=trainImages,\n",
    "    maskPaths=trainMasks,\n",
    "    transforms=transforms\n",
    ")\n",
    "testDataset = SegmentationDataset(\n",
    "    imagePaths=testImages, \n",
    "    maskPaths=testMasks,\n",
    "    transforms=transforms\n",
    ")\n",
    "\n",
    "print(f\"[INFO] found {len(trainDataset)} examples in the training set...\")\n",
    "print(f\"[INFO] found {len(testDataset)} examples in the test set...\")\n",
    "\n",
    "trainLoader = DataLoader(\n",
    "    trainDataset, \n",
    "    shuffle=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "testLoader = DataLoader(\n",
    "    testDataset, \n",
    "    shuffle=False, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23264d22-d880-4b5c-bc0d-db684892f56f",
   "metadata": {},
   "source": [
    "### Init UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7732049a-184f-48a1-a24b-47e84bf5d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet().to(DEVICE)\n",
    "\n",
    "lossFunc = BCEWithLogitsLoss()\n",
    "opt = Adam(unet.parameters(), lr=INIT_LR)\n",
    "\n",
    "trainSteps = len(trainDataset) // BATCH_SIZE\n",
    "testSteps = len(testDataset) // BATCH_SIZE\n",
    "\n",
    "H = {\"train_loss\": [], \"test_loss\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117f20e-aa20-44ad-a3b4-96e053a494c4",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e04fe9-4fcf-4b1b-9cfe-9c813ebdbad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                | 0/40 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training the network...\") \n",
    "startTime = time.time() \n",
    "for e in tqdm(range(NUM_EPOCHS)):\n",
    "    unet.train() \n",
    "\n",
    "    totalTrainLoss = 0 \n",
    "    totalTestLoss = 0 \n",
    "\n",
    "    for (i, (x, y)) in enumerate(trainLoader):\n",
    "        (x, y) = (x.to(DEVICE), y. to(DEVICE))\n",
    "\n",
    "        pred = unet(x)\n",
    "        loss = lossFunc(pred, y) \n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward() \n",
    "        opt.step() \n",
    "\n",
    "        totalTrainLoss += loss \n",
    "\n",
    "    with torch.no_grad(): \n",
    "        unet.eval() \n",
    "\n",
    "        for (x, y) in testLoader: \n",
    "            (x, y) = (x.to(DEVICE), y. to(DEVICE)) \n",
    "\n",
    "            pred = unet(x) \n",
    "            totalTestLoss += lossFunc(pred, y) \n",
    "\n",
    "    avgTrainLoss = totalTrainLoss / trainSteps\n",
    "    avgTestLoss = totalTestLoss / testSteps\n",
    "\n",
    "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "    H[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
    "\n",
    "    print(\"[INFO] EPOCH: {}/{}\".format(e+1, NUM_EPOCHS))\n",
    "    print(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(avgTrainLoss, avgTestLoss))\n",
    "\n",
    "endTime = time.time() \n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb6e2c-ff12-4e2f-a321-8bf8fd4f91ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
